## 데이터 축소
- 방대한 양의 데이터를 대상으로 복잡하게 데이터를 분석하고 마이닝 기법을 적용한다면 매우 많은 시간과 비용이 소요되어 비 현실적이다 -> 데이터 축소 필요
<hr>

# 데이터 축소 전략
- 차원적 축소
  - 데이터 인코딩 스키마를 적용하여 압축 또는 축소된 표현 제공
- 모수적 모형
  - 모집단이 정규분포를 띤다는 가정하에 표본 통계량으로 모집단 통계량을 추정
- 비모수적 모형
  - 위와 다르게 군집화, 표본추출, 히스토그램 등이 대표적인 예
<hr>

# 데이터 큐브
- 다차원 집계 정보
- 원천 데이터를 여러 관점에서 추상화시켜 데이터 축소를 구현
- 사전에 계산되고, 요약된 데이터에 신속히 접근할 수 있도록 하며, 다양한 데이터 분석 처리 가능
<hr>

# 속성 부분집합 선택
- 연관성이 낮거나 중복되는 데이터 속성을 제거하여 데이터 집합의 크기를 줄이는 기법을 의미
- 목표는 전체 속성에 가장 가까운 데이터 범주의 확률 분포와 최소의 속성 집합을 찾는 것
- 소모적 탐색법
- 경험적 방법
<hr>

# 소모적 탐색법
- 2^n 개의 가능한 속성 조합을 모두 탐색
- n이 커질수록 엄청난 비용 발생
<hr>

# 경험적 방법
- 검색 공간을 축소하여 탐색
- 매회마다 최선으로 보이는 것을 선택(greedy)
- 전역적으로 최적이 될 것이라는 기대속에 지역적으로 최적인 해를 선택해가는 방법
- 단계적 전진 선택법
  - 속성의 공집합으로 시작해서 최적의 속성들을 하나씩 추가하는 방법
- 단계적 후진 제거법
  - 속성의 전체집합으로 시작하여 최악의 속성들을 하나씩 제거하는 방법
- 전진 선택법과 후진 제거법의 결합
- 의사결정트리 귀납법
  - 데이터 마이닝 기법중 분류를 위해 고안되었고, 흐름도와 유사한 구조를 가짐
<hr>

# 의사결정트리
- 분류, 회귀 모두 사용가능
- 범주형 또는 연속형 데이터에 대해서 예측가능
- Terminal node 의 수가 분리된 집합의 수
- Terminal node 를 합하면 전체 데이터의 수와 동일
<hr>

# 불순도/ 불확실성
- 분류나무의 경우 순도증가, 불순도 또는 불확실성이 최대한 감소하도록 하는 방향으로 학습시킨다.
- 위와같은 과정을 정보 이론에서는 정보 획득이라고 칭함
<hr>

# 엔트로피
- 불확실성 최소, 순도 최대 -> 0
- 반대로 범주가 2개 뿐이고 해당 개체의 수가 동일하게 반반일 경우, 불확실성 최대, 순도 최소-> 0.5
- 의사결정 트리는 각 영역의 순도 증가, 불확실성 감소시키는 방향으로 학습
<hr>

# 지니계수
- 오분류오차 : 불순도 측정이 가능하지만 미분이 불가능하여 자주 쓰이지 않음
<hr>

# 의사결정 트리 모델 학습
- 재귀적 분귀: 입력 변수 영역을 2개로 구분
- 가지치기: 너무 자세하게 구분된 영역을 통합
  - 분기가 너무 많으면 학습데이터에 과적합이 될 수 있음
  - 의사결정 트리의 분기수가 증가할 때, 처음에는 새로운 데이터에 대한 오분류율이 감소하나 일정 수준 이상이 되면 오분류율이 증가하는 현상일 발생
  - 따라서 가지치기를 수행 (잘라내는 것 보다는 merge의 개념이 더 강함)
<hr>

# 차원 축소
- 원천 데이터의 촉소판을 얻기 위한 데이터 부호화 또는 데이터 변환의 적용
- 웨이블릿 변환
- 주성분 분석
 <hr>
 
# 웨이블릿 변환
- 이산 웨이블릿 변환: 데이터 벡터 X를 다른 수치적 벡터 X`으로 변환
- 웨이블릿 변환 데이터가 원천 데이터와 같은 속성수를 가지지만 데이터 축소로 볼 수 있는 것은 변환 데이터가 압축되어 보이기 때문
- 웨이블릿 계수 중 가장 유력한 일부만을 저장함으로써 데이터 근사치를 유지
- 데이터의 주요 특징들은 보존하면서도 잡을을 제거하는 역할을 하기도 하므로 데이터 정제를 위해서도 효과적임
<hr>

# DWT 적용 절차
<hr>

# 피라미드 알고리즘
<hr>

# 주성분 분석
- n개의 속성을 가진 튜플에 대하여 데이터를 표현하는데 최적으로 사용될 수 있는 n차원 직교벡터들에 대한 k를 찾음 -> 감소된 차원의 공간을 갖는 데이터 공간 생성(차원 축소)
- 주성분 분석 절차
    1. 입력 데이터를 표준화하여 같은 범위에 속하게 함(표준화를 통해 큰 범위를 갖는 속성들이 작은 범위를 갖는 속성들을 압도하지 않도록 하기 위해)
    2. 표준화된 입력 데이터를 위한 기저를 제공하는 직교 벡터들을 계산, 이들을 주 성분이라고 하며, 입력 데이터는 주성분의 선형 조합
    3. 주성분은 중요도의 내림차순으로 정렬
    4. 내림차순 정렬이 되어있어 약한 주성분을 제거함으로써 데이터 크기감소
- 속성의 초기 집합의 부분집합을 유지하며 속성 집합 크기를 줄임
- 필수적인 속성들의 핵심 결합
- 기대하지 않았던 관계를 보여주기도 하여 평범하지 않은 결과 해석이 가능
- 비용이 효과적임
- 순서화되지 않은 속성에 적용 가능
- 희소 데이터와 비대칭 데이터에 모두 적용 가능
- 다차원 데이터를 2차원 데이터로 축소가능
-  결론: 웨이블릿 변환은 고차원 데이터에 적합하고, 희소 데이터 취급에는 PCA가 더 유리하다
<hr>

# 회귀 모형
- 주어진 데이터의 근사치를 구하는데 사용
- 선형회귀는 확률변수 y를 예측변수 x 의 선형함수로 모형화 y = wx + b
- 계수 w, b 는 데이터를 분리하는 실제 선과 그 선의 추정치 사이의 오류를 최소화해주는 최소제곱법에 의해 구할 수 있음
<hr>

# 표본추출
- 큰 데이터 집합을 많은 수의 임의 데이터 샘플로 표현 가능
- 비복원 단순 무작위 표본
  - 임의로 s개의 튜플을 취하는 방법
- 복원 단순 무작위 표본
  - 복원시키며 s개의 튜플을 취하느 방법
- 집략 표본
  - m개의 상호 배반적 군집으로 묶여 있는 가운데 s개의 군집을 단순 무작위로 추출
- 층화 표본
  - 상호 배반적 부분들로 분할되어 있다면, 각 층에서 하나씩 단순 무작위로 추출
<hr>

# 히스토그램
- 구간화를 사용하여 데이터 분포의 근사치를 구하는 데이터 축소의 전형적 형태
- 속성 A의 데이터를 버킷 혹은 빈이라 불리는 분리 집합으로 나눔
- 각 버킷이 단일한 속성 값/ 빈도 쌍으로 표현되기도 하고, 주어진 속성에 대한 연속 범위를 나타내기도 함
- 희소 데이터, 밀집 데이터 모두에 효과적, 비대칭 데이터, 균일한 데이터 모두 매우 효과적
- 단일 속성에 대한 히스토그램에서는 속성 간의 의존성 포착 가능
- 일반적으로 5개 까지의 속성을 가진 데이터의 근사치를 구하는데 효과적이라고 알려짐
- 버킷 결정 및 속성 값 분할 방법
  - 동등 폭
    - 각 버킷의 범위는 균일
  - 동등 빈도
    - 각 버킷의 빈도가 일정
  - V- 최적
    - 최소 분산을 갖는 히스토그램을 의미, 히스토그램 분산은 각 버킷이 나타내는 데이터 값들의 가중합이며, 버킷 가중치는 버킷에 있는 값들의 개수와 동일
  - 최대 차이
    - 인접한 값들의 각 쌍 사이의 차이를 고려, 사용자 정의 버킷의 수에 대하여 x-1개의 최대 차이를 갖는 쌍들에 대한 각 쌍 사이에 버킷 경계가 정해짐
<hr>

# 군집화
- 데이터 튜플을 객체로 간주하고, 각 객체들을 군집이라는 그룹으로 나눔
- 한 군집 내 객체들과는 유사하면서도 다른 군집 내 객체들과는 유사하지 않도록 군집화
- 유사성은 공간 내에서 객체들이 어떻게 가까운지의 관점에 따라 거리 함수에 기반하여 정의
- 품질 = 지름, 지름 = 두 객체 간 최대 거리
- 중심거리의 경우 클러스터 품질로 대체 측정
- 클러스터 지름은 짧을수록 클러스터간 중심거리는 길수록 군집화의 품질이 높다고 볼 수 있음